---
title: "Original PCP, increasing <LOD"
subtitle: "EDC example"
author: "Lizzy Gibson"
date: "10/9/2019"
output:
  html_document:
    toc: yes
    toc_float: yes
    toc_depth: 4
    code_folding: 'hide'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
options(scipen = 999)
library(haven)
library(tidyverse)
library(janitor)
library(reshape2)
library(broom)
library(tableone)
library(knitr)
library(ggcorrplot)
library(ggfortify)  
library(gridExtra)
library(factoextra)
library(Matrix)
library(rlist)
library(ggrepel)
library(broom)
library(RColorBrewer)
library(gplots)
library(grid)
```

## Read Data

```{r read}
library(MNdata)

edc <- mn_edc %>% select(1:18) %>% 
                    rename("TCS" = 17,
                                "BPA" = 18,
                                "BP_3" = 14,
                                "DCP_24" = 11,
                                "DCP_25" = 12,
                                "B_PB" = 13,
                                "M_PB" = 15,
                                "P_PB" = 16,
                                "MECPP" = 2,
                                "MEHHP" = 3,
                                "MEOHP" = 4,
                                "MCPP" = 5,
                                "MIBP" = 6,
                                "MBP" = 7,
                                "MBZP" = 8,
                                "MEP" = 9,
                                "MEHP" = 10) %>% 
  drop_na()

mixture_data <- edc %>%
  mutate_if(is.numeric, scale, center = FALSE, scale = TRUE) %>% 
  select(-sid) %>% 
  drop_na()
```

## Steps

1. Run ORIGINAL PCP on original air pollution data.
2. Artificially assign increasing LOD's (10%, 20%, 30%, 40%, and 50% <LOD) to separate test datasets.
3. Run ORIGINAL PCP version on all 5 <LOD datasets, save L and S matrices from each.
4. Evaluate relative error -- compare results from <LOD datasets with original.

## Create \<LOD Datasets

*Push values \<LOD to zero*

```{r}
# Create version with 10% lowest values for each variable as below the LOD
mix_data_lod_10 <- mixture_data %>% 
  mutate_all(~ifelse(. <= quantile(., probs = .10), -1, .)) %>% as.matrix()

# Create version with 20% lowest values for each variable as below the LOD
mix_data_lod_20 <- mixture_data %>% 
  mutate_all(~ifelse(. <= quantile(., probs = .20), -1, .)) %>% as.matrix()

# Create version with 30% lowest values for each variable as below the LOD
mix_data_lod_30 <- mixture_data %>% 
  mutate_all(~ifelse(. <= quantile(., probs = .30), -1, .)) %>% as.matrix()

# Create version with 40% lowest values for each variable as below the LOD
mix_data_lod_40 <- mixture_data %>% 
  mutate_all(~ifelse(. <= quantile(., probs = .40), -1, .)) %>% as.matrix()

# Create version with 50% lowest values for each variable as below the LOD
mix_data_lod_50 <- mixture_data %>% 
  mutate_all(~ifelse(. <= quantile(., probs = .50), -1, .)) %>% as.matrix()
```

### >LOD PCP Function

```{r import}
prox_l1 <- function(Y, c) {
  
    myzero <- matrix(data = 0, ncol = ncol(Y), nrow = nrow(Y))
    X <- sign(Y) * pmax(abs(Y) - c, myzero, na.rm = TRUE)
    X
    } 

############################################################
############################################################

# Prox nuclear norm function, L1 norm of the singular values
# This encourages matrix to be low rank by pushing SV to zero (sparse)
prox_nuclear <- function(Y,c) {
  
  USV <- svd(Y)
  U <- USV$u
  S <- USV$d
  V <- USV$v

  myzero <- vector("numeric", length = length(S))
  S_new <- sign(S) * pmax(abs(S) - c, myzero, na.rm = TRUE)
  # Threshold the singular values, if SV < c, push it to zero

  X <- U %*% diag(S_new) %*% t(V)
  # % X is the truncation of the original
  # % Multiply the thresholded SVD components back together
  
  nuclearX  <- sum(abs(S_new))
  # This is the L1 norm of the truncated singular values
  # Goes into the loss function

  list(X = X, nuclearX = nuclearX)
  }

############################################################
############################################################

# is same function for convergence criteria
# is the difference among matrices > noise threshold?
## if TRUE, keep iterating, if FALSE, end

# Compares L1, L2, L3 OR S1, S2
# Need ... for function to handle different number of inputs

is_same <- function(SAME_THRESH, ...) {
  flag <- TRUE
  varargin <- list(...)
  
  for (i in 1:(length(as.list(match.call())) - 3)) {
    # match.call returns a call in which all of the specified arguments are specified by their full names.
    # length(as.list( gives the number of function input arguments given in the call
    # for L1, L2, L3, length = 5, (5-3 = 2), two comparisons, L1/L2, and L2/L3
    # for S1, S2, length = 4, (4-3 = 1), one comparison = 1 loop
    if (max(abs(varargin[[i]] - varargin[[i + 1]])) > SAME_THRESH) {
        flag <- FALSE
      }
    }
  flag
}

############################################################
############################################################

# loss_lod function (only used in the loss function)

loss_lod <- function(X, D, Delta) {
  # % D is the original data
  # % X is the new thing (L + S)
  # Delta is the LOD
  
  # % Pointwise boolean operation tricks for element-wise updating
  X_lod <- (X - D)     * (D >= 0) +
  # % D>=0 will spit out 0/1 (no/yes)
  # % If D_ij >= 0, then X_lod = (X - D)_ij, else zero
  # Normal loss for >LOD measures (distance from original value)
    
           (X - Delta) * (D < 0 & X > Delta) +
  # % If D_ij < 0 AND X_ij > Delta, then X_lod = X_ij - Delta, else zero
  # % D is < 0 when < LOD
  # This should be penalized more because D <LOD but (L+S) >LOD (distance from LOD)
    
            X          * (D < 0 & X < 0)
  # % If D_ij < 0 AND X_ij < 0, then X_lod = X, else zero
  
  l <- sum(X_lod^2) / 2
  # % L2 norm
  
  # % Any D_ij < 0 AND X_ij < Delta AND > 0 are treated as equal
  
  # % Minimize discrepancy for valid data
  # % Want to shrink negative things
  l
  }

############################################################
############################################################

# % If the LOD threshold Delta = 0, solve the following ADMM splitting problem:
#   % min_{L1,L2,L3,S1,S2}
# %      ||L1||_* + lambda * ||S1||_1 + mu/2 * ||L2+S2-D||_F^2 + I_{L3>=0}
# % s.t. L1 = L2
# %      L1 = L3
# %      S1 = S2.
# %
# % If Delta is not 0, replace ||L2+S2-D||_F^2 with LOD penalty.
# %
# % Below-LOD data input in D should be denoted as negative values, e.g. -1.

lod_pcp <- function(D, lambda, mu, Delta) {
  
  m <- nrow(D)
  n <- ncol(D)
  rho <- 1 # Augmented Lagrangian coefficient (rate)
  
  L1 <- matrix(0, m, n)
  L2 <- matrix(0, m, n)
  L3 <- matrix(0, m, n)
  
  S1 <- matrix(0, m, n)
  S2 <- matrix(0, m, n)
  
  Z1 <- matrix(0, m, n)
  Z2 <- matrix(0, m, n)
  Z3 <- matrix(0, m, n)
  
  # Max iteration
  MAX_ITER <- 100
  
  # Convergence Thresholds
  LOSS_THRESH <- 1e-5
  SAME_THRESH <- 1e-4
  
  loss <- vector("numeric", MAX_ITER)
  
  for (i in 1:MAX_ITER) {
    
    nuc <- prox_nuclear( ((L2 + L3 - (Z1 + Z2)/rho)/2), 1/2/rho)
    L1 <- nuc[[1]]
    nuclearL1 <- nuc[[2]] #nuclearX
    # % L, Z, S all start at zero, and change each iteration
    # % Prox_nuc is singular value thresholding
    # % L is low rank matrix

    S1 <- prox_l1((S2 - Z3/rho), lambda/rho)
    # % S is sparse matrix
    
    # These are all derivatives
    L2_opt1 <- (mu*rho*D     + (mu + rho)*Z1 - mu*Z3 + (mu + rho)*rho*L1 - mu*rho*S1) / (2*mu*rho + rho^2)
    L2_opt2 <- L1 + Z1/rho
    L2_opt3 <- (mu*rho*Delta + (mu + rho)*Z1 - mu*Z3 + (mu + rho)*rho*L1 - mu*rho*S1) / (2*mu*rho + rho^2)
    L2_opt4 <- (               (mu + rho)*Z1 - mu*Z3 + (mu + rho)*rho*L1 - mu*rho*S1) / (2*mu*rho + rho^2)
    
    L2 <-     (L2_opt1 * (D >= 0)) +
      # If D >= LOD, use opt1 (Good)
              (L2_opt2 * ((D < 0) & ((L2 + S2) >= 0) & ((L2 + S2) <= Delta))) +
      # If D < LOD and new is between 0 and LOD, use opt2 (Good)
              (L2_opt3 * ((D < 0) & ((L2 + S2) > Delta))) +
      # If D < LOD and new > LOD use opt3 (Bad)
              (L2_opt4 * ((D < 0) & ((L2 + S2) < 0)))
      # If D < LOD and new < LOD, use opt4 (Bad)
      # % L2_new becomes whichever of the 4 meets the conditions
    
    S2_opt1 <- (mu*rho*D     + (mu + rho)*Z3 - (mu*Z1) + (mu + rho)*rho*S1 - mu*rho*L1) / (2*mu*rho + rho^2)
    S2_opt2 <- S1 + (Z3/rho)
    S2_opt3 <- (mu*rho*Delta + (mu + rho)*Z3 - (mu*Z1) + (mu + rho)*rho*S1 - mu*rho*L1) / (2*mu*rho + rho^2)
    S2_opt4 <- (               (mu + rho)*Z3 - (mu*Z1) + (mu + rho)*rho*S1 - mu*rho*L1) / (2*mu*rho + rho^2)
    
    S2 <- (S2_opt1 * (D >= 0)) +
          (S2_opt2 * ((D < 0) & ((L2 + S2) >= 0) & ((L2 + S2) <= Delta))) +
          (S2_opt3 * ((D < 0) & ((L2 + S2) > Delta))) +
          (S2_opt4 * ((D < 0) & ((L2 + S2) < 0)))
    # % For data >LOD, use opt 1
    # % S2 becomes whichever of the 4 meets the conditions
    
    # % The code block above takes LOD into account.
    # % The code block commented out below does not take LOD into account
    # %     L2 = (mu*rho*D + (mu+rho)*Z1 - mu*Z3 + (mu+rho)*rho*L1 - mu*rho*S1) / (2*mu*rho+rho^2);
    # %     S2 = (mu*rho*D + (mu+rho)*Z3 - mu*Z1 + (mu+rho)*rho*S1 - mu*rho*L1) / (2*mu*rho+rho^2);
    
    L3 <- pmax(L1 + Z2/rho, 0, na.rm = TRUE)
    # % Non-Negativity constraint!
    
    Z1 <- Z1 + rho*(L1 - L2)
    Z2 <- Z2 + rho*(L1 - L3)
    Z3 <- Z3 + rho*(S1 - S2)
    # % Z accumulate differnces between L and L and between S and S
    
    loss[i] <- nuclearL1 + 
      (lambda*sum(abs(S1))) +
      (mu*loss_lod((L2 + S2), D, Delta)) +
      sum(Z1*(L1 - L2)) +
      sum(Z2*(L1 - L3)) +
      sum(Z3*(S1 - S2)) +
      (rho/2 * (sum((L1-L2)^2) + sum((L1 - L3)^2) + sum((S1 - S2)^2)))
    # % The code block above takes LOD into account.
    
    # % The code block commented out below does not take LOD into account
    # %     loss(i) = nuclearL1 + lambda*sum(sum(abs(S1))) + mu/2*sum(sum((L2+S2-D).^2)) ...
    # %         + sum(sum(Z1.*(L1-L2))) + sum(sum(Z2.*(L1-L3))) + sum(sum(Z3.*(S1-S2))) ...
    # %         + rho/2 * ( sum(sum((L1-L2).^2)) + sum(sum((L1-L3).^2)) + sum(sum((S1-S2).^2)) );

    if ((i != 1) && 
        (abs(loss[i-1] - loss[i]) < LOSS_THRESH) && 
        is_same(SAME_THRESH, L1, L2, L3) &&
        is_same(SAME_THRESH, S1, S2)) {
      break} # % Convergence criteria!
  }
  
  L <- L3 # (L1 + L2 + L3) / 3
  S <- (S1 + S2) / 2
  list(L = L, S = S, loss = loss)
}
```

## Run PCP

Run >LOD PCP on separate datasets.

```{r}
m <- nrow(mixture_data)

results_0  <- lod_pcp(as.matrix(mixture_data), 4/sqrt(m), 0.3, 0)
results_10 <- lod_pcp(mix_data_lod_10, 4/sqrt(m), 0.3, 0)
results_20 <- lod_pcp(mix_data_lod_20, 4/sqrt(m), 0.3, 0)
results_30 <- lod_pcp(mix_data_lod_30, 4/sqrt(m), 0.3, 0)
results_40 <- lod_pcp(mix_data_lod_40, 4/sqrt(m), 0.3, 0)
results_50 <- lod_pcp(mix_data_lod_50, 4/sqrt(m), 0.3, 0)

L_lod0 <- results_0[[1]]
S_lod0 <- results_0[[2]]
L_lod10 <- results_10[[1]]
S_lod10 <- results_10[[2]]
L_lod20 <- results_20[[1]]
S_lod20 <- results_20[[2]]
L_lod30 <- results_30[[1]]
S_lod30 <- results_30[[2]]
L_lod40 <- results_40[[1]]
S_lod40 <- results_40[[2]]
L_lod50 <- results_50[[1]]
S_lod50 <- results_50[[2]]
```

## Compare Results

### X - L - S

```{r}
X <- as.matrix(mixture_data)

F_norm <- as_tibble(cbind(lod0 = norm((X - L_lod0 - S_lod0), type = "F")/norm((X), type = "F"),
      lod10 = norm((X - L_lod10 - S_lod10), type = "F")/norm((X), type = "F"),
      lod20 = norm((X - L_lod20 - S_lod20), type = "F")/norm((X), type = "F"),
      lod30 = norm((X - L_lod30 - S_lod30), type = "F")/norm((X), type = "F"),
      lod40 = norm((X - L_lod40 - S_lod40), type = "F")/norm((X), type = "F"),
      lod50 = norm((X - L_lod50 - S_lod50), type = "F")/norm((X), type = "F"))) %>% 
  gather(percent_blod, norm) %>% 
  mutate(Matrix = "Both")

F_norm_s <- as_tibble(cbind(lod0 = norm((X - S_lod0), type = "F")/norm((X), type = "F"),
      lod10 = norm((X - S_lod10), type = "F")/norm((X), type = "F"),
      lod20 = norm((X - S_lod20), type = "F")/norm((X), type = "F"),
      lod30 = norm((X - S_lod30), type = "F")/norm((X), type = "F"),
      lod40 = norm((X - S_lod40), type = "F")/norm((X), type = "F"),
      lod50 = norm((X - S_lod50), type = "F")/norm((X), type = "F"))) %>% 
  gather(percent_blod, norm) %>% 
  mutate(Matrix = "Sparse")

F_norm_l <- as_tibble(cbind(lod0 = norm((X - L_lod0), type = "F")/norm((X), type = "F"),
      lod10 = norm((X - L_lod10), type = "F")/norm((X), type = "F"),
      lod20 = norm((X - L_lod20), type = "F")/norm((X), type = "F"),
      lod30 = norm((X - L_lod30), type = "F")/norm((X), type = "F"),
      lod40 = norm((X - L_lod40), type = "F")/norm((X), type = "F"),
      lod50 = norm((X - L_lod50), type = "F")/norm((X), type = "F"))) %>% 
  gather(percent_blod, norm) %>% 
  mutate(Matrix = "Low-Rank")

F_norm %>% ggplot(aes(x = percent_blod, y = norm)) + 
  geom_point() + geom_path(aes(group = 1)) + theme_bw() +
  labs(x = "Percent Below LOD", y = "Relative Error\n(norm(X-L-S) / norm(X))")

rbind(F_norm, F_norm_l, F_norm_s) %>% ggplot(aes(x = percent_blod, y = norm)) + 
  geom_point(aes(color = Matrix)) + geom_path(aes(group = Matrix, color = Matrix)) + theme_bw() +
  labs(x = "Percent Below LOD", y = "Relative Error\n(norm(X - .) / norm(X))")
```

### Individual Solution Matrices

```{r}
L_diff <- as_tibble(cbind(lod0 = norm((L_lod0 - L_lod0), type = "F")/norm((L_lod0), type = "F"),
      lod10 = norm((L_lod10 - L_lod0), type = "F")/norm((L_lod0), type = "F"),
      lod20 = norm((L_lod20 - L_lod0), type = "F")/norm((L_lod0), type = "F"),
      lod30 = norm((L_lod30 - L_lod0), type = "F")/norm((L_lod0), type = "F"),
      lod40 = norm((L_lod40 - L_lod0), type = "F")/norm((L_lod0), type = "F"),
      lod50 = norm((L_lod50 - L_lod0), type = "F")/norm((L_lod0), type = "F"))) %>% 
  gather(percent_blod, norm) %>% 
  mutate(Matrix = "Low-Rank")

S_diff <- as_tibble(cbind(lod0 = norm((S_lod0 - S_lod0), type = "F")/norm((S_lod0), type = "F"),
      lod10 = norm((S_lod10 - S_lod0), type = "F")/norm((S_lod0), type = "F"),
      lod20 = norm((S_lod20 - S_lod0), type = "F")/norm((S_lod0), type = "F"),
      lod30 = norm((S_lod30 - S_lod0), type = "F")/norm((S_lod0), type = "F"),
      lod40 = norm((S_lod40 - S_lod0), type = "F")/norm((S_lod0), type = "F"),
      lod50 = norm((S_lod50 - S_lod0), type = "F")/norm((S_lod0), type = "F"))) %>% 
  gather(percent_blod, norm) %>% 
  mutate(Matrix = "Sparse")

rbind(L_diff, S_diff) %>% ggplot(aes(x = percent_blod, y = norm)) + 
  geom_point(aes(color = Matrix)) + geom_path(aes(group = Matrix, color = Matrix)) + theme_bw() +
  labs(x = "Percent Below LOD", y = "Relative Error in Solution Matrices\n(norm(difference) / norm(lod0))")
```

### SVD low rank vs. no \<LOD

```{r}
# Extract right singular vectors from each low rank solution matrix
V_lod0  <- svd(L_lod0)$v[,1:5]
V_lod10 <- svd(L_lod10)$v[,1:5]
V_lod20 <- svd(L_lod20)$v[,1:5]
V_lod30 <- svd(L_lod30)$v[,1:5]
V_lod40 <- svd(L_lod40)$v[,1:5]
V_lod50 <- svd(L_lod50)$v[,1:5]

V_diff <- as_tibble(cbind(lod0 =norm((V_lod0 - V_lod0), type = "F")/norm((V_lod0), type = "F"),
      lod10 =norm((V_lod10 - V_lod0), type = "F")/norm((V_lod0), type = "F"),
      lod20 =norm((V_lod20 - V_lod0), type = "F")/norm((V_lod0), type = "F"),
      lod30 =norm((V_lod30 - V_lod0), type = "F")/norm((V_lod0), type = "F"),
      lod40 =norm((V_lod40 - V_lod0), type = "F")/norm((V_lod0), type = "F"),
      lod50 =norm((V_lod50 - V_lod0), type = "F")/norm((V_lod0), type = "F"))) %>% 
  gather(percent_blod, norm)

V_diff %>% ggplot(aes(x = percent_blod, y = norm)) + 
  geom_point() + geom_path(aes(group = 1)) + theme_bw() +
  labs(x = "Percent Below LOD", 
       y = "Relative Error in Low-Rank Solution Singular Vectors\n(norm(SV - SV_lod0) / norm(SV_lod0))")
```

### SVD low rank vs. original

```{r}
# Extract right singular vectors from original matrix
V_orig <- svd(X)$v[,1:5]

V_diff <- as_tibble(cbind(lod0 =norm((V_lod0 - V_orig), type = "F")/norm((V_orig), type = "F"),
      lod10 =norm((V_lod10 - V_orig), type = "F")/norm((V_orig), type = "F"),
      lod20 =norm((V_lod20 - V_orig), type = "F")/norm((V_orig), type = "F"),
      lod30 =norm((V_lod30 - V_orig), type = "F")/norm((V_orig), type = "F"),
      lod40 =norm((V_lod40 - V_orig), type = "F")/norm((V_orig), type = "F"),
      lod50 =norm((V_lod50 - V_orig), type = "F")/norm((V_orig), type = "F"))) %>% 
  gather(percent_blod, norm)

V_diff %>% ggplot(aes(x = percent_blod, y = norm)) + 
  geom_point() + geom_path(aes(group = 1)) + theme_bw() +
  labs(x = "Percent Below LOD", 
       y = "Relative Error in Low-Rank Solution Singular Vectors\n(norm(SV - SV_orig) / norm(SV_orig))")
```

### Values \<LOD vs \>LOD

* Are values \<LOD estimated more poorly than those \>LOD? Are \<LOD driving the increased error?
* Rescale by range/variance of values \<LOD.
    * Wider range -> more error

```{r}
# This one is all zeros bc nothing is <LOD to begin with
tf_lod_0 <- mixture_data %>% 
  mutate_all(~ifelse(. == -1, TRUE, FALSE))
l_blod_0 <- as.matrix(tf_lod_0*L_lod0)
s_blod_0 <- as.matrix(tf_lod_0*S_lod0)
true_0 <- as.matrix(mixture_data*tf_lod_0)


tf_lod_10 <- mix_data_lod_10 %>% 
  as_tibble() %>% 
  mutate_all(~ifelse(. == -1, TRUE, FALSE))
# T/F dataset, T = <LOD
l_blod_10 <- as.matrix(tf_lod_10*L_lod10)
s_blod_10 <- as.matrix(tf_lod_10*S_lod10)
# keep <LOD predictions, >LOD predictions are zero
# Solution L matrix with >LOD values as zero
true_10 <- as.matrix(mixture_data*tf_lod_10)
# keep 10% lowest true values, push >LOD values to zero
# Original matrix with only bottom 10percent, all >LOD set to zero

tf_lod_20 <- mix_data_lod_20 %>% 
    as_tibble() %>% 
  mutate_all(~ifelse(. == -1, TRUE, FALSE))
l_blod_20 <- as.matrix(tf_lod_20*L_lod20)
s_blod_20 <- as.matrix(tf_lod_20*S_lod20)
true_20 <- as.matrix(mixture_data*tf_lod_20)

tf_lod_30 <- mix_data_lod_30 %>% 
    as_tibble() %>% 
  mutate_all(~ifelse(. == -1, TRUE, FALSE))
l_blod_30 <- as.matrix(tf_lod_30*L_lod30)
s_blod_30 <- as.matrix(tf_lod_30*S_lod30)
true_30 <- as.matrix(mixture_data*tf_lod_30)

tf_lod_40 <- mix_data_lod_40 %>% 
    as_tibble() %>% 
  mutate_all(~ifelse(. == -1, TRUE, FALSE))
l_blod_40 <- as.matrix(tf_lod_40*L_lod40)
s_blod_40 <- as.matrix(tf_lod_40*S_lod40)
true_40 <- as.matrix(mixture_data*tf_lod_40)

tf_lod_50 <- mix_data_lod_50 %>% 
    as_tibble() %>% 
  mutate_all(~ifelse(. == -1, TRUE, FALSE))
l_blod_50 <- as.matrix(tf_lod_50*L_lod50)
s_blod_50 <- as.matrix(tf_lod_50*S_lod50)
true_50 <- as.matrix(mixture_data*tf_lod_50) 

#Subtract true values and divide by norm of true
less_diff <- as_tibble(cbind(lod0 = norm((true_0 - l_blod_0 - s_blod_0), type = "F"), #/norm((true_0), type = "F"),
      lod10 = norm((true_10 - l_blod_10 - s_blod_10), type = "F")/norm((true_10), type = "F"),
      lod20 = norm((true_20 - l_blod_20 - s_blod_20), type = "F")/norm((true_20), type = "F"),
      lod30 = norm((true_30 - l_blod_30 - s_blod_30), type = "F")/norm((true_30), type = "F"),
      lod40 = norm((true_40 - l_blod_40 - s_blod_40), type = "F")/norm((true_40), type = "F"),
      lod50 = norm((true_50 - l_blod_50 - s_blod_50), type = "F")/norm((true_50), type = "F"))) %>% 
  gather(percent_blod, norm) %>% 
  mutate(Values = "< LOD")
```

```{r}
above_lod_0 <- mixture_data %>% 
  mutate_all(~ifelse(. == -1, FALSE, TRUE))
l_above_blod_0 <- as.matrix(above_lod_0*L_lod0)
s_above_blod_0 <- as.matrix(above_lod_0*S_lod0)
above_true_0 <- as.matrix(mixture_data*above_lod_0)

above_lod_10 <- mix_data_lod_10 %>% 
    as_tibble() %>% 
  mutate_all(~ifelse(. == -1, FALSE, TRUE))
# T/F dataset, T = >LOD
l_above_blod_10 <- as.matrix(above_lod_10*L_lod10)
s_above_blod_10 <- as.matrix(above_lod_10*S_lod10)
# keep >LOD predictions, BLOD predictions are zero
above_true_10 <- as.matrix(mixture_data*above_lod_10)
# keep 10% lowest true values, push >LOD values to zero

above_lod_20 <- mix_data_lod_20 %>% 
    as_tibble() %>% 
  mutate_all(~ifelse(. == -1, FALSE, TRUE))
l_above_blod_20 <- as.matrix(above_lod_20*L_lod20)
s_above_blod_20 <- as.matrix(above_lod_20*S_lod20)
above_true_20 <- as.matrix(mixture_data*above_lod_20)

above_lod_30 <- mix_data_lod_30 %>% 
    as_tibble() %>% 
  mutate_all(~ifelse(. == -1, FALSE, TRUE))
l_above_blod_30 <- as.matrix(above_lod_30*L_lod30)
s_above_blod_30 <- as.matrix(above_lod_30*S_lod30)
above_true_30 <- as.matrix(mixture_data*above_lod_30)

above_lod_40 <- mix_data_lod_40 %>% 
    as_tibble() %>% 
  mutate_all(~ifelse(. == -1, FALSE, TRUE))
l_above_blod_40 <- as.matrix(above_lod_40*L_lod40)
s_above_blod_40 <- as.matrix(above_lod_40*S_lod40)
above_true_40 <- as.matrix(mixture_data*above_lod_40)

above_lod_50 <- mix_data_lod_50 %>% 
    as_tibble() %>% 
  mutate_all(~ifelse(. == -1, FALSE, TRUE))
l_above_blod_50 <- as.matrix(above_lod_50*L_lod50)
s_above_blod_50 <- as.matrix(above_lod_50*S_lod50)
above_true_50 <- as.matrix(mixture_data*above_lod_50) 
```

```{r}
#divide by true values
above_diff <- as_tibble(cbind(
  lod0  = norm((above_true_0  - l_above_blod_0  - s_above_blod_0),  type = "F")/norm(above_true_0,  type = "F"),
  lod10 = norm((above_true_10 - l_above_blod_10 - s_above_blod_10), type = "F")/norm(above_true_10, type = "F"),
  lod20 = norm((above_true_20 - l_above_blod_20 - s_above_blod_20), type = "F")/norm(above_true_20, type = "F"),
  lod30 = norm((above_true_30 - l_above_blod_30 - s_above_blod_30), type = "F")/norm(above_true_30, type = "F"),
  lod40 = norm((above_true_40 - l_above_blod_40 - s_above_blod_40), type = "F")/norm(above_true_40, type = "F"),
  lod50 = norm((above_true_50 - l_above_blod_50 - s_above_blod_50), type = "F")/norm(above_true_50, type = "F"))) %>% 
  gather(percent_blod, norm) %>% 
  mutate(Values = "> LOD")

rbind(above_diff, less_diff) %>% 
  ggplot(aes(x = percent_blod, y = norm)) + 
  geom_point(aes(color = Values)) + geom_path(aes(group = Values, color = Values)) + theme_bw() +
  labs(x = "Percent Below LOD", 
       y = "Relative Error in Values < LOD & > LOD\n(norm(X-L-S) / norm(X))")
```
