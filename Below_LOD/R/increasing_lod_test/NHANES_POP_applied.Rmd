---
title: "PCP-LOD: Increasing LOD in NHANES POPs"
author: "Lizzy Gibson"
date: "2/1/2020"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("tidyverse")
library(tidyverse)
#install.packages("janitor")
library(janitor)
#install.packages("ggcorrplot")
library(ggcorrplot)
#install.packages("ggfortify")
library(ggfortify)  
#install.packages("gridExtra")
library(gridExtra)
#install.packages("factoextra")
library(factoextra)
#install.packages("knitr")
library(knitr)
library(haven)
library(rlist)

options(scipen = 999)
```

# Data Import and Cleaning

CDC imputes values \<LOD as LOD/sqrt(2). Here, if comment code = "below detectable limit", set value to -1.

```{r}
pop_lods <- read_sas(here::here("./Data/studypop_lod.sas7bdat")) %>% 
  clean_names() %>% 
  mutate(lbx074la = ifelse(lbd074lc == 1, -1, lbx074la),
         lbx099la = ifelse(lbd099lc == 1, -1, lbx099la),
         lbx105la = ifelse(lbd105lc == 1, -1, lbx105la),
         lbx118la = ifelse(lbd118lc == 1, -1, lbx118la),
         lbx138la = ifelse(lbd138lc == 1, -1, lbx138la),
         lbx153la = ifelse(lbd153lc == 1, -1, lbx153la),
         lbx156la = ifelse(lbd156lc == 1, -1, lbx156la),
         lbx157la = ifelse(lbd157lc == 1, -1, lbx157la),
         lbx167la = ifelse(lbd167lc == 1, -1, lbx167la),
         lbx170la = ifelse(lbd170lc == 1, -1, lbx170la),
         lbx180la = ifelse(lbd180lc == 1, -1, lbx180la),
         lbx187la = ifelse(lbd187lc == 1, -1, lbx187la),
         lbxd01la = ifelse(lbdd01lc == 1, -1, lbxd01la),
         lbxd03la = ifelse(lbdd03lc == 1, -1, lbxd03la),
         lbxd04la = ifelse(lbdd04lc == 1, -1, lbxd04la),
         lbxd05la = ifelse(lbdd05lc == 1, -1, lbxd05la),
         lbxd07la = ifelse(lbdd07lc == 1, -1, lbxd07la),
         lbxf01la = ifelse(lbdf01lc == 1, -1, lbxf01la),
         lbxf02la = ifelse(lbdf02lc == 1, -1, lbxf02la),
         lbxf03la = ifelse(lbdf03lc == 1, -1, lbxf03la),
         lbxf04la = ifelse(lbdf04lc == 1, -1, lbxf04la),
         lbxf05la = ifelse(lbdf05lc == 1, -1, lbxf05la),
         lbxf06la = ifelse(lbdf06lc == 1, -1, lbxf06la),
         lbxf07la = ifelse(lbdf07lc == 1, -1, lbxf07la),
         lbxf08la = ifelse(lbdf08lc == 1, -1, lbxf08la),
         lbxpcbla = ifelse(lbdpcblc == 1, -1, lbxpcbla),
         lbxtcdla = ifelse(lbdtcdlc == 1, -1, lbxtcdla),
         lbxhxcla = ifelse(lbdhxclc == 1, -1, lbxhxcla),
         lbx189la = ifelse(lbd189lc == 1, -1, lbx189la),
         lbx194la = ifelse(lbd194lc == 1, -1, lbx194la),
         lbx196la = ifelse(lbd196lc == 1, -1, lbx196la),
         lbx199la = ifelse(lbd199lc == 1, -1, lbx199la),
         lbxd02la = ifelse(lbdd02lc == 1, -1, lbxd02la),
         lbxf09la = ifelse(lbdf09lc == 1, -1, lbxf09la)) %>% 
    select(names(.)[grep("la", names(.))] ) %>% 
    na.omit(.) %>% 
    as.matrix(.)
```

Here, make matrix of LODs to use in PCP-LOD. Comment code = 1 means "below detectable limit", so element-wise multiplication gives us a matrix of LOD/sqrt(2) where values are \<LOD and zero where values are \>LOD. Take this product and multiply by sqrt(2) and we have a matrix of LODs with zero in cells that are above the LOD. Should we still have the LOD there? Will it matter?

```{r}
lods <- read_sas(here::here("./Data/studypop_lod.sas7bdat")) %>% 
  clean_names() %>% 
  select(names(.)[grep("lc", names(.))] ) %>% 
  na.omit()

pops <- read_sas(here::here("./Data/studypop_lod.sas7bdat")) %>% 
  clean_names() %>% 
  select(names(.)[grep("la", names(.))] ) %>% 
  na.omit()

lod_matrix <- (lods * pops) * sqrt(2)
lod_matrix <- as.matrix(lod_matrix)
```

**Need to scale LODs, too!**

Need to scale PCBs because they have super different ranges. Need to get rid of values less than LOD to scale and then add them back.



```{r}
summary(pop_lods)

# make <LOD NA so they dont affect the scaling
pop_lods[pop_lods < 0] <- NA

pop_sd <- apply(as.matrix(pop_lods), 2, sd, na.rm = TRUE)
pop_sd[is.na(pop_sd)] <- 1
pop_sd <- as.vector(pop_sd)

head(pop_lods[,5]/pop_sd[5])

# Scale bc way different ranges
pop_lods <- scale(pop_lods, center = FALSE, scale = pop_sd)

# make <LOD negative again
pop_lods[is.na(pop_lods)] <- -100
mixture_data <- pop_lods

summary(mixture_data)
```

Also want to scale the LODS!

```{r}
# Scale bc way different ranges
lod_matrix <- scale(lod_matrix, center = FALSE, scale = pop_sd)
```

Probably don't want log.

```{r}
#create a new data frame with my.x and convert values for the predictor variables to its natural log.
#log.x <- data.frame(apply(studypop, 2, FUN = function(x) log(x))) %>%
 # setNames(paste(my.x, "log", sep = ".")) #add suffix 12 to the predictors' name

#check dimensions of dataset
#dim(log.x)

#summary statistics on dataset
#summary(log.x)
```

### >LOD PCP Function

```{r import}
# Updated 10/16/2019
# Prox L1 norm function, soft thresholding
# if Y < c (threshold), push to zero
prox_l1 <- function(Y, c) {
  
    myzero <- matrix(data = 0, ncol = ncol(Y), nrow = nrow(Y))
    X <- sign(Y) * pmax(abs(Y) - c, myzero, na.rm = TRUE)
    X
    } 

############################################################
############################################################

# Prox nuclear norm function, L1 norm of the singular values
# This encourages matrix to be low rank by pushing SV to zero (sparse)
prox_nuclear <- function(Y,c) {
  
  USV <- svd(Y)
  U <- USV$u
  S <- USV$d
  V <- USV$v

  myzero <- vector("numeric", length = length(S))
  S_new <- sign(S) * pmax(abs(S) - c, myzero, na.rm = TRUE)
  # Threshold the singular values, if SV < c, push it to zero

  X <- U %*% diag(S_new) %*% t(V)
  # % X is the truncation of the original
  # % Multiply the thresholded SVD components back together
  
  nuclearX  <- sum(abs(S_new))
  # This is the L1 norm of the truncated singular values
  # Goes into the loss function

  list(X = X, nuclearX = nuclearX)
  }

############################################################
############################################################

# is same function for convergence criteria
# is the difference among matrices > noise threshold?
## if TRUE, keep iterating, if FALSE, end

# Compares L1, L2, L3 OR S1, S2
# Need ... for function to handle different number of inputs
# length(varargin) gives the number of function input arguments given in the call
# for L1, L2, L3, THREE comparisons, L1/L2, L1/L3, and L2/L3
# for S1, S2, one comparison
is_same <- function(SAME_THRESH, ...) {
  flag <- TRUE
  varargin <- list(...)
    if (length(varargin) == 2) {
      if (max(abs(varargin[[1]] - varargin[[2]])) > SAME_THRESH) {
      flag <- FALSE
    }
  }
    else if (length(varargin) == 3) {
      if ((max(abs(varargin[[1]] - varargin[[2]])) > SAME_THRESH) |
         (max(abs(varargin[[1]] - varargin[[3]])) > SAME_THRESH) |
         (max(abs(varargin[[2]] - varargin[[3]])) > SAME_THRESH)) {
      flag <- FALSE
      }
    }
  flag
}

############################################################
############################################################

# loss_lod function (only used in the loss function)

loss_lod <- function(X, D, Delta) {
  # % D is the original data
  # % X is the new thing (L + S)
  # Delta is the LOD
  
  # % Pointwise boolean operation tricks for element-wise updating
  X_lod <- (X - D)     * (D >= 0) +
  # % D>=0 will spit out 0/1 (no/yes)
  # % If D_ij >= 0, then X_lod = (X - D)_ij, else zero
  # Normal loss for >LOD measures (distance from original value)
    
           ((X) - Delta) * (D < 0 & ((X) > Delta)) +
  # % If D_ij < 0 AND X_ij > Delta, then X_lod = X_ij - Delta, else zero
  # % D is < 0 when < LOD
  # This should be penalized more because D <LOD but (L+S) >LOD (distance from LOD)
    
            X          * (D < 0 & X < 0)
  # % If D_ij < 0 AND X_ij < 0, then X_lod = X, else zero
  
  l <- sum(X_lod^2) / 2
  # % L2 norm
  
  # % Any D_ij < 0 AND X_ij < Delta AND > 0 are treated as equal
  
  # % Minimize discrepancy for valid data
  # % Want to shrink negative things
  l
}

############################################################
############################################################

# % If the LOD threshold Delta = 0, solve the following ADMM splitting problem:
#   % min_{L1,L2,L3,S1,S2}
# %      ||L1||_* + lambda * ||S1||_1 + mu/2 * ||L2+S2-D||_F^2 + I_{L3>=0}
# % s.t. L1 = L2
# %      L1 = L3
# %      S1 = S2.
# %
# % If Delta is not 0, replace ||L2+S2-D||_F^2 with LOD penalty.
# %
# % Below-LOD data input in D should be denoted as negative values, e.g. -1.

lod_pcp <- function(D, lambda, mu, Delta) {
  
  m <- nrow(D)
  n <- ncol(D)
  rho <- 1 # Augmented Lagrangian coefficient (rate)
  
  L1 <- matrix(0, m, n)
  L2 <- matrix(0, m, n)
  L3 <- matrix(0, m, n)
  
  S1 <- matrix(0, m, n)
  S2 <- matrix(0, m, n)
  
  Z1 <- matrix(0, m, n)
  Z2 <- matrix(0, m, n)
  Z3 <- matrix(0, m, n)
  
  # Max iteration
  MAX_ITER <- 1000
  
  # Convergence Thresholds
  LOSS_THRESH <- 1e-5
  SAME_THRESH <- 1e-4
  
  loss <- vector("numeric", MAX_ITER)
  
  for (i in 1:MAX_ITER) {
    
    nuc <- prox_nuclear( ((L2 + L3 - (Z1 + Z2)/rho)/2), 1/2/rho)
    L1 <- nuc[[1]]
    nuclearL1 <- nuc[[2]] #nuclearX
    # % L, Z, S all start at zero, and change each iteration
    # % Prox_nuc is singular value thresholding
    # % L is low rank matrix

    S1 <- prox_l1((S2 - Z3/rho), lambda/rho)
    # % S is sparse matrix
    
    # These are all derivatives
    L2_opt1 <- (mu*rho*D     + (mu + rho)*Z1 - mu*Z3 + (mu + rho)*rho*L1 - mu*rho*S1) / (2*mu*rho + rho^2)
    L2_opt2 <- L1 + Z1/rho
    L2_opt3 <- ((mu*rho*Delta + (((mu + rho)*Z1) - (mu*Z3) + ((mu + rho)*rho*L1) - (mu*rho*S1)))) / ((2*mu*rho) + (rho^2))
    L2_opt4 <- (               (mu + rho)*Z1 - mu*Z3 + (mu + rho)*rho*L1 - mu*rho*S1) / (2*mu*rho + rho^2)
    
    L2_new <- (L2_opt1 * (D >= 0)) +
      # If D >= LOD, use opt1 (Good)
              (L2_opt2 * ((D < 0) & ((L2 + S2) >= 0) & ((L2 + S2) <= Delta))) +
      # If D < LOD and new is between 0 and LOD, use opt2 (Good)
              (L2_opt3 * ((D < 0) & ((L2 + S2) > Delta))) +
      # If D < LOD and new > LOD use opt3 (Bad)
              (L2_opt4 * ((D < 0) & ((L2 + S2) < 0)))
      # If D < LOD and new < LOD, use opt4 (Bad)
      # % L2_new becomes whichever of the 4 meets the conditions
    
    S2_opt1 <- (mu*rho*D     + (mu + rho)*Z3 - (mu*Z1) + (mu + rho)*rho*S1 - mu*rho*L1) / (2*mu*rho + rho^2)
    S2_opt2 <- S1 + (Z3/rho)
    S2_opt3 <- (((mu*rho*Delta) + (((mu + rho)*Z3) - (mu*Z1) + ((mu + rho)*rho*S1) - (mu*rho*L1)))) / ((2*mu*rho) + (rho^2))
    S2_opt4 <- (               (mu + rho)*Z3 - (mu*Z1) + (mu + rho)*rho*S1 - mu*rho*L1) / (2*mu*rho + rho^2)
    
    S2 <- (S2_opt1 * (D >= 0)) +
          (S2_opt2 * ((D < 0) & ((L2 + S2) >= 0) & ((L2 + S2) <= Delta))) +
          (S2_opt3 * ((D < 0) & ((L2 + S2) > Delta))) +
          (S2_opt4 * ((D < 0) & ((L2 + S2) < 0)))
    # % For data >LOD, use opt 1
    # % S2 becomes whichever of the 4 meets the conditions
    
    L2 <- L2_new
    # % The code block above takes LOD into account.
    # % The code block commented out below does not take LOD into account
    # %     L2 = (mu*rho*D + (mu+rho)*Z1 - mu*Z3 + (mu+rho)*rho*L1 - mu*rho*S1) / (2*mu*rho+rho^2);
    # %     S2 = (mu*rho*D + (mu+rho)*Z3 - mu*Z1 + (mu+rho)*rho*S1 - mu*rho*L1) / (2*mu*rho+rho^2);
    
    L3 <- pmax(L1 + Z2/rho, 0, na.rm = TRUE)
    # % Non-Negativity constraint!
    
    Z1 <- Z1 + rho*(L1 - L2)
    Z2 <- Z2 + rho*(L1 - L3)
    Z3 <- Z3 + rho*(S1 - S2)
    # % Z accumulate differnces between L and L and between S and S
    
    loss[i] <- nuclearL1 + 
      (lambda*sum(abs(S1))) +
      (mu*loss_lod((L2 + S2), D, Delta)) +
      sum(Z1*(L1 - L2)) +
      sum(Z2*(L1 - L3)) +
      sum(Z3*(S1 - S2)) +
      (rho/2 * (sum((L1-L2)^2) + sum((L1 - L3)^2) + sum((S1 - S2)^2)))
    # % The code block above takes LOD into account.
    
    # % The code block commented out below does not take LOD into account
    # %     loss(i) = nuclearL1 + lambda*sum(sum(abs(S1))) + mu/2*sum(sum((L2+S2-D).^2)) ...
    # %         + sum(sum(Z1.*(L1-L2))) + sum(sum(Z2.*(L1-L3))) + sum(sum(Z3.*(S1-S2))) ...
    # %         + rho/2 * ( sum(sum((L1-L2).^2)) + sum(sum((L1-L3).^2)) + sum(sum((S1-S2).^2)) );

    print(str_c("Iteration: ", i, "; Obj: ", round(loss[i], 7)))
    
    if ((i != 1) && 
        (abs(loss[i-1] - loss[i]) < LOSS_THRESH) && 
        is_same(SAME_THRESH, L1, L2, L3) &&
        is_same(SAME_THRESH, S1, S2)) {
      break} # % Convergence criteria!
  }
  
  L <- (L1 + L2 + L3) / 3 #L3
  S <- (S1 + S2) / 2 #S1
  list(L = L, S = S, loss = loss)
}
```

## Run PCP

```{r}
m <- nrow(mixture_data)
n <- ncol(mixture_data)

results_0  <- lod_pcp(mixture_data, 1/sqrt(m), 1/sqrt(n), Delta = lod_matrix)
results_0$loss

L_lod0 <- results_0[[1]]
S_lod0 <- results_0[[2]]
svd(L_lod0)$d

length(L_lod0[L_lod0 >= 0])
length(L_lod0)
```

## Vary $\mu$

Create function to output new low rank matrix, sparse matrix, singular values and rank of new low rank matrix, and $\lambda$ and $\mu$ parameters used

```{r mu_lam}
mu_value <- c(seq(0.01, 1/sqrt(n), by = 0.1), seq(1/sqrt(n), 10, by = 0.5))

make_L_mu <- function(mu){ 
  mixture_mu <- lod_pcp(as.matrix(mixture_data), 1/sqrt(m), mu, as.matrix(lod_matrix))
  L <- mixture_mu$L
  S <- mixture_mu$S
  sv_diag <- svd(L)$d # singular values on new low rank matrix
  sv_count <- sum(sv_diag > 0.001) # rank of new low rank matrix
  list(L = L, SV = sv_diag, S = S, Count = sv_count, Mu = mu, Lambda = 1/sqrt(m))
}
```

Loop over $\lambda$, $\mu$ pairs.

```{r looop, results = FALSE, cache = TRUE}
range_out <- map(.x = mu_value, ~make_L_mu(mu = .x))
```

### Changing Low Rank Matrix

Examine how rank of low rank matrix changes with varying $\lambda$ and $\mu$.

```{r rank_ml}
singular_value_count <- range_out %>% list.map(.[4]) %>% unlist() %>% 
  cbind(sv_count = ., mu = mu_value) %>% as_tibble() 
```

```{r rank2}
singular_value_count %>% summary()
```

```{r}
singular_value_count %>% 
  ggplot(aes(x = mu, y = sv_count)) +
  geom_point() + geom_line() +
  theme_bw() +
  labs(y = "Rank",
       x = expression(mu),
       title = expression(paste("Changing rank of low rank matrix with varying ", mu))) +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 70, hjust = 1))
```

### Changing Sparse Matrix

Examine how sparsity of sparse matrix changes with varying $\lambda$ and $\mu$.

```{r mu_sparse2}
sparse <- range_out %>% list.map(.[3])
# Extract all sparse matrices

loop.vector2 <- 1:length(mu_value)

cells <- m*n
prop_not_zero <- vector(length = length(mu_value))

for (i in loop.vector2) { # Loop over loop.vector2

 not_zeros <- sum(sparse[[i]]$S != 0)
  # Create proportion of non-zero values across all cells in sparse matrix
  prop_not_zero[i] <- not_zeros/cells
}

sparseness <- cbind(prop_not_zero, singular_value_count) %>% as_tibble()
```

```{r plot_sparse2}
sparseness %>% summary()
```

```{r}
sparseness %>%
  ggplot(aes(x = mu, y = prop_not_zero)) +
  geom_point() + geom_line() +
  theme_bw() +
  labs(y = "Proportion Non-Zero",
       x = expression(mu),
       title = expression(paste("Changing sparsity of sparse matrix with varying ", mu))) +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 70, hjust = 1))
```

## Error

Each PCP creates a low rank L matrix and a sparse matrix that sum to the original D matrix with some error. To choose the best value for $\mu$, we add the L and S matrices to form D', then subtract D - D' to get an error matrix for each $\mu$. We then take the RMSE of the low rank matrix and the F norm of each error matrix, divided by the norm of the original matrix, to get relative error.

```{r cv_both}
# low_rank and sparse are lists of L and S matrices for each 
mix <- as.matrix(mixture_data)

loop.vector2 <- 1:length(mu_value)
rmse <- vector(length = length(mu_value))
rel_error <- vector(length = length(mu_value))
new_ml <- list()

low_rank_ml <- range_out %>% list.map(.[1])
sparse_ml <- range_out %>% list.map(.[3])

for (i in loop.vector2) { # Loop over loop.vector
  new_ml[[i]]  <- low_rank_ml[[i]]$L + sparse_ml[[i]]$S
  rmse[i]      <- sqrt(mean((mix - new_ml[[i]])^2))
  rel_error[i] <- norm((mixture_data - new_ml[[i]]), type = "F")/norm(mix, type = "F")
  }

cv <- cbind(mu_value, rmse, rel_error) %>% as_tibble()
```

### Plot Error

```{r plot_both}
cv %>% summary()
```

Below is the cross-section at $\lambda = 1/\sqrt{n}$, same as only varying $\mu$.

```{r}
cv %>% 
  ggplot(aes(y = rmse, x = mu_value)) + 
  geom_point() + geom_line() +
  theme_bw() +
  labs(y = "Root Mean Squared Error",
       x = expression(mu),
       title = expression(paste("Changing RMSE with varying ", mu)))
```
