---
title: "<LOD PCP with EDCs"
author: "Lizzy Gibson"
date: "2/26/2019"
output:
  html_document:
    toc: yes
    toc_float: yes
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
options(scipen = 999)
library(haven)
library(tidyverse)
library(janitor)
library(reshape2)
library(broom)
library(tableone)
library(knitr)
library(ggcorrplot)
library(ggfortify)  
library(gridExtra)
library(factoextra)
library(Matrix)
library(rlist)
library(ggrepel)
library(broom)
library(RColorBrewer)
library(gplots)
library(grid)
library(matconv)
```

## Read Data

```{r read}
library(MNdata)

edc <- mn_edc %>% select(1:18) %>% 
                    rename("TCS" = 17,
                                "BPA" = 18,
                                "BP_3" = 14,
                                "DCP_24" = 11,
                                "DCP_25" = 12,
                                "B_PB" = 13,
                                "M_PB" = 15,
                                "P_PB" = 16,
                                "MECPP" = 2,
                                "MEHHP" = 3,
                                "MEOHP" = 4,
                                "MCPP" = 5,
                                "MIBP" = 6,
                                "MBP" = 7,
                                "MBZP" = 8,
                                "MEP" = 9,
                                "MEHP" = 10) %>% 
  drop_na()
```

### Import PCP Function

```{r import}
# Prox L1 norm function, soft thresholding
# if Y < c (threshold), push to zero
prox_l1 <- function(Y, c) {
  
  myzero <- matrix(data = 0, ncol = ncol(Y), nrow = nrow(Y))
  X <- sign(Y) * pmax(abs(Y) - c, myzero, na.rm = TRUE)
  X
} 

############################################################
############################################################

# Prox nuclear norm function, L1 norm of the singular values
# This encourages matrix to be low rank by pushing SV to zero (sparse)
prox_nuclear <- function(Y,c) {
  
  USV <- svd(Y)
  U <- USV$u
  S <- USV$d
  V <- USV$v
  
  myzero <- vector("numeric", length = length(S))
  S_new <- sign(S) * pmax(abs(S) - c, myzero, na.rm = TRUE)
  # Threshold the singular values, if SV < c, push it to zero
  
  X <- U %*% diag(S_new) %*% t(V)
  # % X is the truncation of the original
  # % Multiply the thresholded SVD components back together
  
  nuclearX  <- sum(abs(S_new))
  # This is the L1 norm of the truncated singular values
  # Goes into the loss function
  
  list(X = X, nuclearX = nuclearX)
}

############################################################
############################################################

# is same function for convergence criteria
# is the difference among matrices > noise threshold?
## if TRUE, keep iterating, if FALSE, end

# Compares L1, L2, L3 OR S1, S2
# Need ... for function to handle different number of inputs
# length(varargin) gives the number of function input arguments given in the call
# for L1, L2, L3, THREE comparisons, L1/L2, L1/L3, and L2/L3
# for S1, S2, one comparison
is_same <- function(SAME_THRESH, ...) {
  flag <- TRUE
  varargin <- list(...)
  if (length(varargin) == 2) {
    if (max(abs(varargin[[1]] - varargin[[2]])) > SAME_THRESH) {
      flag <- FALSE
    }
  }
  else if (length(varargin) == 3) {
    if ((max(abs(varargin[[1]] - varargin[[2]])) > SAME_THRESH) |
        (max(abs(varargin[[1]] - varargin[[3]])) > SAME_THRESH) |
        (max(abs(varargin[[2]] - varargin[[3]])) > SAME_THRESH)) {
      flag <- FALSE
    }
  }
  flag
}

############################################################
############################################################

# loss_lod function (only used in the loss function)

loss_lod <- function(X, D, Delta) {
  # % D is the original data
  # % X is the new thing (L + S)
  # Delta is the LOD
  
  # % Pointwise boolean operation tricks for element-wise updating
  X_lod <- (X - D)     * (D >= 0) +
    # % D>=0 will spit out 0/1 (no/yes)
    # % If D_ij >= 0, then X_lod = (X - D)_ij, else zero
    # Normal loss for >LOD measures (distance from original value)
    
    t(t(X) - Delta) * (D < 0 & t(t(X) > Delta)) +
    # % If D_ij < 0 AND X_ij > Delta, then X_lod = X_ij - Delta, else zero
    # % D is < 0 when < LOD
    # This should be penalized more because D <LOD but (L+S) >LOD (distance from LOD)
    
    X          * (D < 0 & X < 0)
  # % If D_ij < 0 AND X_ij < 0, then X_lod = X, else zero
  
  l <- sum(X_lod^2) / 2
  # % L2 norm
  
  # % Any D_ij < 0 AND X_ij < Delta AND > 0 are treated as equal
  
  # % Minimize discrepancy for valid data
  # % Want to shrink negative things
  l
}

############################################################
############################################################

# % If the LOD threshold Delta = 0, solve the following ADMM splitting problem:
#   % min_{L1,L2,L3,S1,S2}
# %      ||L1||_* + lambda * ||S1||_1 + mu/2 * ||L2+S2-D||_F^2 + I_{L3>=0}
# % s.t. L1 = L2
# %      L1 = L3
# %      S1 = S2.
# %
# % If Delta is not 0, replace ||L2+S2-D||_F^2 with LOD penalty.
# %
# % Below-LOD data input in D should be denoted as negative values, e.g. -1.

pcp_lod <- function(D, lambda, mu, Delta) {
  
  m <- nrow(D)
  n <- ncol(D)
  rho <- 1 # Augmented Lagrangian coefficient (rate)
  
  L1 <- matrix(0, m, n)
  L2 <- matrix(0, m, n)
  L3 <- matrix(0, m, n)
  
  S1 <- matrix(0, m, n)
  S2 <- matrix(0, m, n)
  
  Z1 <- matrix(0, m, n)
  Z2 <- matrix(0, m, n)
  Z3 <- matrix(0, m, n)
  
  # Max iteration
  MAX_ITER <- 100
  
  # Convergence Thresholds
  LOSS_THRESH <- 1e-5
  SAME_THRESH <- 1e-4
  
  loss <- vector("numeric", MAX_ITER)
  
  for (i in 1:MAX_ITER) {
    
    nuc <- prox_nuclear( ((L2 + L3 - (Z1 + Z2)/rho)/2), 1/2/rho)
    L1 <- nuc[[1]]
    nuclearL1 <- nuc[[2]] #nuclearX
    # % L, Z, S all start at zero, and change each iteration
    # % Prox_nuc is singular value thresholding
    # % L is low rank matrix
    
    S1 <- prox_l1((S2 - Z3/rho), lambda/rho)
    # % S is sparse matrix
    
    # These are all derivatives
    L2_opt1 <- (mu*rho*D     + (mu + rho)*Z1 - mu*Z3 + (mu + rho)*rho*L1 - mu*rho*S1) / (2*mu*rho + rho^2)
    L2_opt2 <- L1 + Z1/rho
    L2_opt3 <- t((mu*rho*Delta + t(((mu + rho)*Z1) - (mu*Z3) + ((mu + rho)*rho*L1) - (mu*rho*S1)))) / ((2*mu*rho) + (rho^2))
    L2_opt4 <- (               (mu + rho)*Z1 - mu*Z3 + (mu + rho)*rho*L1 - mu*rho*S1) / (2*mu*rho + rho^2)
    
    L2_new <- (L2_opt1 * (D >= 0)) +
      # If D >= LOD, use opt1 (Good)
      (L2_opt2 * ((D < 0) & ((L2 + S2) >= 0) & t(t(L2 + S2) <= Delta))) +
      # If D < LOD and new is between 0 and LOD, use opt2 (Good)
      (L2_opt3 * ((D < 0) & t(t(L2 + S2) > Delta))) +
      # If D < LOD and new > LOD use opt3 (Bad)
      (L2_opt4 * ((D < 0) & ((L2 + S2) < 0)))
    # If D < LOD and new < LOD, use opt4 (Bad)
    # % L2_new becomes whichever of the 4 meets the conditions
    
    S2_opt1 <- (mu*rho*D     + (mu + rho)*Z3 - (mu*Z1) + (mu + rho)*rho*S1 - mu*rho*L1) / (2*mu*rho + rho^2)
    S2_opt2 <- S1 + (Z3/rho)
    S2_opt3 <- t(((mu*rho*Delta) + t(((mu + rho)*Z3) - (mu*Z1) + ((mu + rho)*rho*S1) - (mu*rho*L1)))) / ((2*mu*rho) + (rho^2))
    S2_opt4 <- (               (mu + rho)*Z3 - (mu*Z1) + (mu + rho)*rho*S1 - mu*rho*L1) / (2*mu*rho + rho^2)
    
    S2 <- (S2_opt1 * (D >= 0)) +
      (S2_opt2 * ((D < 0) & ((L2 + S2) >= 0) & t(t(L2 + S2) <= Delta))) +
      (S2_opt3 * ((D < 0) & t(t(L2 + S2) > Delta))) +
      (S2_opt4 * ((D < 0) & ((L2 + S2) < 0)))
    # % For data >LOD, use opt 1
    # % S2 becomes whichever of the 4 meets the conditions
    
    L2 <- L2_new
    # % The code block above takes LOD into account.
    # % The code block commented out below does not take LOD into account
    # %     L2 = (mu*rho*D + (mu+rho)*Z1 - mu*Z3 + (mu+rho)*rho*L1 - mu*rho*S1) / (2*mu*rho+rho^2);
    # %     S2 = (mu*rho*D + (mu+rho)*Z3 - mu*Z1 + (mu+rho)*rho*S1 - mu*rho*L1) / (2*mu*rho+rho^2);
    
    L3 <- pmax(L1 + Z2/rho, 0, na.rm = TRUE)
    # % Non-Negativity constraint!
    
    Z1 <- Z1 + rho*(L1 - L2)
    Z2 <- Z2 + rho*(L1 - L3)
    Z3 <- Z3 + rho*(S1 - S2)
    # % Z accumulate differnces between L and L and between S and S
    
    loss[i] <- nuclearL1 + 
      (lambda*sum(abs(S1))) +
      (mu*loss_lod((L2 + S2), D, Delta)) +
      sum(Z1*(L1 - L2)) +
      sum(Z2*(L1 - L3)) +
      sum(Z3*(S1 - S2)) +
      (rho/2 * (sum((L1-L2)^2) + sum((L1 - L3)^2) + sum((S1 - S2)^2)))
    # % The code block above takes LOD into account.
    
    # % The code block commented out below does not take LOD into account
    # %     loss(i) = nuclearL1 + lambda*sum(sum(abs(S1))) + mu/2*sum(sum((L2+S2-D).^2)) ...
    # %         + sum(sum(Z1.*(L1-L2))) + sum(sum(Z2.*(L1-L3))) + sum(sum(Z3.*(S1-S2))) ...
    # %         + rho/2 * ( sum(sum((L1-L2).^2)) + sum(sum((L1-L3).^2)) + sum(sum((S1-S2).^2)) );
    
    if ((i != 1) && 
        (abs(loss[i-1] - loss[i]) < LOSS_THRESH) && 
        is_same(SAME_THRESH, L1, L2, L3) &&
        is_same(SAME_THRESH, S1, S2)) {
      break} # % Convergence criteria!
  }
  
  L <- L3 # (L1 + L2 + L3) / 3
  S <- S1 #(S1 + S2) / 2
  list(L = L, S = S, loss = loss)
}
```

## PCP with Selected $\mu = 0.3$

```{r basic}
mix <- edc %>%
  mutate_if(is.numeric, scale, center = FALSE, scale = TRUE) %>% 
  select(-sid) %>% 
  drop_na() %>% 
  as.matrix(.)

summary(mix)

m <- nrow(mix)
n <- ncol(mix)

svd(mix)$d

#BY CONVENTION, should we change?
lambda = 1/sqrt(m)
```

```{r new}
#mu from validation RMD
mixture_out.4 <- pcp_lod(mix, lambda, 0.3, 0)
# PCP function now with 100 interations -> different answer

summary(mixture_out.4)

mixture_S.4 <- mixture_out.4$S
mixture_L.4 <- mixture_out.4$L

svd(mixture_L.4)$d
#This L IS low rank
```

### Variance Explained

```{r}
#prcomp function does this: s$d <- s$d / sqrt(max(1, nrow(x) - 1))
#this should be the same as the sum of the eigenvalues
sum(matrix(svd(mix)$d/sqrt(max(1, nrow(mix) - 1)))^2)
#If mix matrix isn't scaled, this sum does not equal 17

#As mu increases, penalty goes to zero, and variance in low rank matrix goes to 17
sum(matrix(svd(mixture_L.4)$d/sqrt(max(1, nrow(mixture_L.4) - 1)))^2)
sum(matrix(svd(mixture_S.4)$d/sqrt(max(1, nrow(mixture_S.4) - 1)))^2)

(sum(matrix(svd(mixture_L.4)$d/sqrt(max(1, nrow(mixture_L.4) - 1)))^2) + sum(matrix(svd(mixture_S.4)$d/sqrt(max(1, nrow(mixture_S.4) - 1)))^2))/17
#First 5 SV explain 81% of the variance

(sum(matrix(svd(mixture_L.4)$d/sqrt(max(1, nrow(mixture_L.4) - 1)))^2))/17 

(sum(matrix(svd(mixture_S.4)$d/sqrt(max(1, nrow(mixture_S.4) - 1)))^2))/17

```

## PCP Viz

### Sparse Matrix

sparse matrix to identify unusual, unique, or extreme exposure events.

```{r sparse2}
mixture_S.4 %>% as_tibble() %>% 
  mutate(id = 1:nrow(mixture_S.4)) %>% 
  select(id, everything()) %>% 
  gather(key = exposure, value = value, -id) %>%
  ggplot(aes(x = exposure, y = id)) +
  geom_tile(aes(fill = value)) + 
  scale_fill_gradient2(low = "navy", mid = "blue", high = "yellow", 
                       na.value = "transparent") +
  labs(x = "Exposure", y = "Participant", title = "Sparse matrix of rare events", legend = "Magnitude") + 
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom")
```

```{r}
colfunc <- colorRampPalette(c("blue", "orange"))

### This is not the real heatmap.2 function ###
#png("./Figures/sparse_pcp_matrix.png", height = 1200, width = 1500, res = 250)
heatmap.2(mixture_S.4, key.title = "", key.xlab = "Scaled Concentrations",
          ylab = "Participants", symbreaks = FALSE,
          labRow = FALSE, dendrogram = "none", 
          col = colfunc(100), keysize = 1.5,
          density.info="none", trace="none")
#dev.off()
```

#### Sparsity

```{r}
not_zeros <- sum(mixture_S.4 != 0)
cells <- nrow(mixture_S.4)*ncol(mixture_S.4)
prop_not_zero <- not_zeros/cells
1 - prop_not_zero

mixture_S.4 %>% as_tibble() %>% 
  filter_all(., all_vars(. == 0))

summary(mixture_S.4)

mixture_S.4 %>% as_tibble() %>% 
  filter(MEHP != 0 & MECPP!= 0 & MEHHP != 0 & MEOHP != 0 & MIBP != 0) %>% 
  select(MECPP, MEHHP, MEOHP, MEHP, MIBP)

mixture_S.4 %>% as_tibble() %>% 
  filter(MEHP > 0 & MECPP > 0 & MEHHP > 0 & MEOHP > 0 & MIBP > 0) %>% 
  select(MECPP, MEHHP, MEOHP, MEHP, MIBP)
```

### Low Rank Matrix

low-rank matrix to identify consistent patterns of exposure across the pollutants.

```{r low2}
lr_plot <- as_tibble(svd(mixture_L.4)$v) %>% 
  mutate(edc = c(colnames(edc)[2:18])) %>% 
  select(edc, everything()) %>% 
  gather(key = singular_vector, value = magnitude, V1:V17) %>%
  filter(singular_vector %in% c("V1", "V2", "V3", "V4", "V5")) %>%
  mutate(singular_vector = fct_recode(singular_vector, "Component 1" = "V1",
                                      "Component 2" = "V2",
                                      "Component 3" = "V3",
                                      "Component 4" = "V4",
                                      "Component 5" = "V5")) %>% 
  mutate(Group = case_when(edc == "TCS" | edc == "BPA" ~ "Phenols", 
                           grepl("PB", edc) ~ "Parabens",
                           grepl("_", edc) ~ "Phenols",
                           grepl("^M", edc) == TRUE ~ "Phthalates")) %>% 
  mutate(edc = fct_relevel(edc, "TCS",
                                "BPA",
                                "BP_3",
                                "DCP_24",
                                "DCP_25",
                                "B_PB",
                                "M_PB",
                                "P_PB",
                                "MECPP", 
                                "MEHHP",
                                "MEOHP",
                                "MCPP",
                                "MIBP",
                                "MBP", 
                                "MBZP",  
                                "MEP", 
                                "MEHP")) %>% 
  ggplot(aes(x = edc, y = magnitude, color = Group)) + geom_point() + 
  geom_segment(aes(xend = edc, yend = 0)) +
  facet_wrap(. ~ singular_vector) +
  geom_hline(yintercept = 0, linetype = "dashed", 
             color = "red") +
  theme_bw(base_size = 12) + labs(x = "", y = "Magnitude", title = "") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        strip.background = element_rect(fill = "white")) +
  geom_hline(yintercept = 0, size = 0.2) + 
  theme(legend.position = c(0.85, 0.10), 
                legend.background = element_rect(fill = "white", colour = NA),
        legend.text=element_text(size=15),
        legend.title = element_blank(),
        strip.text.x = element_text(size = 13),
        axis.title.y = element_text(size = 18))

#png("./lr_plot.png", width = 2500, height = 1500, res = 275)
lr_plot
#dev.off()
```

## Component-wise Correlation

Q: Is this the matrix we care about? **svd(mixture_L.4)$u**

```{r}
pcp_scores <- svd(mixture_L.4)$u %>% as_tibble() %>% 
  select(V1:V5) %>% 
  cbind(., sid = edc$sid) %>% 
  as_tibble() %>% 
  mutate(sid = as.character(sid))

pcp_scores %>% select(V1:V5) %>% cor()

#write_csv(pcp_scores, "./Data/pcp_scores.csv")
```

## More Sparsity

```{r}
edc <- edc %>% mutate_if(is.numeric, scale) #so both are scaled

dat_sparse <- mixture_S.4 %>% as_tibble() %>% 
  mutate(sid = edc$sid) %>% 
  left_join(edc, ., by = "sid") # .y extension means from the sparse matrix, .x extension means original values

plot_sparse <- function(x, y, i) {
  dat_sparse %>% 
  ggplot(aes(x = x, y = y)) + geom_point() +
  geom_abline(intercept = 0, slope = max(y)/max(x), color = "red") + 
  # slope = delta y / delta x
  labs(x = "", y = "",
       title = toupper(str_sub(colnames(dat_sparse)[i], start = 1L, end = -3L))) + 
    theme_bw() +
    theme(plot.title = element_text(hjust = 0.5),
          plot.margin = unit(c(.1,.1,.1,.1), "cm"))
}

#Create lists of components and products
observed <- dat_sparse[, 2:18]
observed <- lapply(seq_len(ncol(observed)), function(i) observed[,i])

patterns <- dat_sparse[, 19:35]
patterns <- lapply(seq_len(ncol(patterns)), function(i) patterns[,i])

num <- 2:18

#empty list to put plot output for each pair
out <- list()

#loop through lm for each pattern/product combo
for (i in 1:length(observed)) {
  out[[i]] <- plot_sparse(x = as.matrix(observed[[i]]), y = as.matrix(patterns[[i]]), i = num[i])
}

out[[1]]

grid.arrange(grobs = out,
             left = textGrob("Sparse matrix values", rot = 90, vjust = 1),
             bottom = textGrob("Observed values"))
```

```{r}
sparse_viz <- dat_sparse %>% 
  ggplot(aes(x = MECPP.x, y = MECPP.y)) + geom_point() +
  geom_abline(intercept = 0, slope = max(dat_sparse$MECPP.y)/max(dat_sparse$MECPP.x), color = "red") + 
  # slope = delta y / delta x
  labs(x = "Observed Scaled Concentrations", y = "Sparse Matrix Scaled Concentrations",
       title = "") + 
    theme_bw()

#png("./Figures/sparse_viz.png", width = 800, height = 800, res = 200)
sparse_viz
#dev.off()
```

